{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576048d0-0aca-4fea-999d-70ab12fca289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from scipy.io.matlab import loadmat\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017fdcbd-6bfd-47be-9328-af5358c03b71",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ca788-609c-47cc-8755-2ca61589d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_idx3_ubyte(file_path='../data/t10k-images.idx3-ubyte'):\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        \n",
    "        magic_number = int.from_bytes(f.read(4), 'big')\n",
    "        num_items = int.from_bytes(f.read(4), 'big')\n",
    "\n",
    "        if magic_number == 2051:  # Fichier d'images\n",
    "            num_rows = int.from_bytes(f.read(4), 'big')\n",
    "            num_cols = int.from_bytes(f.read(4), 'big')\n",
    "            data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "            data = data.reshape(num_items, num_rows, num_cols)\n",
    "            \n",
    "        elif magic_number == 2049:  # Fichier d'étiquettes\n",
    "            data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Format de fichier .idx3-ubyte non reconnu\")\n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "def lire_alpha_digit(file_path='../data/binaryalphadigs.mat',caractere=['0']):\n",
    "    \n",
    "    assert caractere, \"List not empty\"\n",
    "\n",
    "    elements = [str(i) for i in range(10)] + list(string.ascii_uppercase)\n",
    "    \n",
    "    index_caractere_list = [elements.index(c) for c in caractere if c in elements]\n",
    "    if not index_caractere_list:\n",
    "        raise ValueError(\"One or many caracters are not recognized.\")\n",
    "\n",
    "    data = loadmat(file_path)\n",
    "    \n",
    "    size_img = data['dat'][0][0].shape\n",
    "    nb_pixel = size_img[0]*size_img[1]\n",
    "    \n",
    "\n",
    "    X = data['dat'][np.array(index_caractere_list)]\n",
    "    X = np.concatenate(X)\n",
    "    X = np.concatenate(X).reshape((X.shape[0],nb_pixel))\n",
    "\n",
    "    return X, size_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870f2cb4-b8fe-479f-817d-39095fcb21ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(X, size_img):\n",
    "    num_images = len(X)\n",
    "    \n",
    "    # Calculate the number of rows and columns for the subplots\n",
    "    cols = np.ceil(np.sqrt(num_images)) \n",
    "    rows = np.ceil(num_images / cols)     \n",
    "    \n",
    "    fig, axes = plt.subplots(int(rows), int(cols), figsize=(cols * 2, rows * 2))\n",
    "    axes = axes.flatten() \n",
    "    \n",
    "    for i, image in enumerate(X):\n",
    "        image = image.reshape(size_img)\n",
    "        axes[i].imshow(image, cmap='gray')\n",
    "    \n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66919e98-0d29-46f9-b1b0-1568f67b695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, size_img = lire_alpha_digit('../data/binaryalphadigs.mat',caractere=['A'])\n",
    "plot_images(X, size_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a3193-d947-4d00-98a4-d057cd6cf053",
   "metadata": {},
   "source": [
    "# RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f0b25-8aec-4c2d-bb90-c7fd67fe9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6d6966-cb25-46fe-8076-8b8f711853f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    def __init__(self, p, q):\n",
    "        self.a = np.zeros(p)\n",
    "        self.b = np.zeros(q)\n",
    "        self.W = np.random.normal(size=(p, q)) * np.sqrt(0.01)\n",
    "\n",
    "    def entree_sortie_RBM(self, V):\n",
    "        return sigmoid(V @ self.W + self.b) \n",
    "\n",
    "    def sortie_entree_RBM(self, H):\n",
    "        return sigmoid(H @ self.W.T + self.a)\n",
    "\n",
    "    def train_RBM(self, X, learning_rate, len_batch, n_epochs,verbose=1):\n",
    "        p, q = self.W.shape\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "    \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            \n",
    "            for ith_batch in range(0, n_samples, len_batch):\n",
    "                \n",
    "                X_batch = X_shuffled[ith_batch:ith_batch + len_batch]\n",
    "                m = X_batch.shape[0]\n",
    "\n",
    "                #Contrastive-Divergence-1 algorithm to estimate the gradient\n",
    "                V0 = X_batch.copy()\n",
    "                \n",
    "                pH_V0 = self.entree_sortie_RBM(V0)\n",
    "                # draw from pH_V0\n",
    "                H0 = (np.random.rand(m, q) < pH_V0) * 1\n",
    "                \n",
    "                pV_H0 = self.sortie_entree_RBM(H0)\n",
    "                # draw from pV_H0\n",
    "                V1 = (np.random.rand(m, p) < pV_H0) * 1\n",
    "                \n",
    "                pH_V1 = self.entree_sortie_RBM(V1)\n",
    "\n",
    "                grad_a = np.sum(V0 - V1, axis=0)\n",
    "                grad_b = np.sum(pH_V0 - pH_V1, axis=0)\n",
    "                grad_W = V0.T @ pH_V0 - V1.T @ pH_V1\n",
    "\n",
    "                self.a += learning_rate * grad_a\n",
    "                self.b += learning_rate * grad_b\n",
    "                self.W += learning_rate * grad_W\n",
    "\n",
    "            # Reconstruction's loss\n",
    "            H = self.entree_sortie_RBM(X_shuffled)\n",
    "            X_rec = self.sortie_entree_RBM(H)\n",
    "            loss = np.mean((X_shuffled - X_rec) ** 2) #quadratic norm\n",
    "\n",
    "            if epoch % 10 == 0 and verbose: # verbose for progression bar\n",
    "                print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def generer_image_RBM(self, nb_images, nb_iter):\n",
    "        p, q = self.W.shape\n",
    "        images = []\n",
    "        \n",
    "        for i in range(nb_images):  # Gibbs\n",
    "            v = (np.random.rand(p) < 0.5) * 1\n",
    "            for j in range(nb_iter):\n",
    "                h = (np.random.rand(q) < self.entree_sortie_RBM(v)) * 1\n",
    "                v = (np.random.rand(p) < self.sortie_entree_RBM(h)) * 1\n",
    "            images.append(v)\n",
    "            \n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019486b4-a330-46aa-9e34-bebaa5839200",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, size_img = lire_alpha_digit('../data/binaryalphadigs.mat', caractere=['A'])\n",
    "\n",
    "nb_features = size_img[0] * size_img[1]\n",
    "p, q = nb_features, 100\n",
    "rbm = RBM(p, q)  # Instance of RBM\n",
    "\n",
    "rbm.train_RBM(X, learning_rate=10 ** (-2), len_batch=10, n_epochs=1000, verbose=0)\n",
    "\n",
    "generated_images = rbm.generer_image_RBM(nb_images=10, nb_iter=200)\n",
    "plot_images(generated_images, size_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35068d-58fb-45ff-a7bf-9153040d21c1",
   "metadata": {},
   "source": [
    "# DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc7d26-e280-4acb-9331-a8004331f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN:\n",
    "    def __init__(self, dbn_size):\n",
    "        \"\"\"\n",
    "        :param dbn_size: List [] of numbers of neurons per layer.\n",
    "        \"\"\"\n",
    "        self.dbn_size = dbn_size\n",
    "        self.rbms = []  # List to store RBMs\n",
    "\n",
    "        # Initialize RBMs for each pair of consecutive layers\n",
    "        for l in range(len(dbn_size) - 1):\n",
    "            p = dbn_size[l]\n",
    "            q = dbn_size[l + 1]\n",
    "            rbm = RBM(p, q)\n",
    "            self.rbms.append(rbm)\n",
    "\n",
    "    def train_DBN(self, X, learning_rate, len_batch, n_epochs, verbose=1):\n",
    "        \n",
    "        tmp = X.copy()\n",
    "        \n",
    "        #Greedy layer wise procedure\n",
    "        for l in range(len(self.dbn_size) - 1):\n",
    "            if verbose:\n",
    "                print(f\"Train RBM {l+1}/{len(self.dbn_size)-1}\\t\") \n",
    "            self.rbms[l].train_RBM(tmp, learning_rate, len_batch, n_epochs,verbose)\n",
    "            tmp = self.rbms[l].entree_sortie_RBM(tmp)\n",
    "\n",
    "\n",
    "    def generer_image_DBN(self, nb_images, nb_iter):\n",
    "        images = []\n",
    "\n",
    "        for i in range(nb_images):\n",
    "\n",
    "            #Gibbs sur la derniere couche cachée\n",
    "            top_rbm = self.rbms[-1]\n",
    "            p, q = top_rbm.W.shape\n",
    "            \n",
    "            # Initialisation aléatoire de la couche visible\n",
    "            v = (np.random.rand(p) < 0.5) * 1\n",
    "\n",
    "            for _ in range(nb_iter):\n",
    "                h = (np.random.rand(q) < top_rbm.entree_sortie_RBM(v)) * 1\n",
    "                v = (np.random.rand(p) < top_rbm.sortie_entree_RBM(h)) * 1\n",
    "\n",
    "            # Propagation vers l'arrière (caché -> visible)\n",
    "            h = v\n",
    "            for rbm in reversed(self.rbms[:-1]):\n",
    "                p, q = rbm.W.shape\n",
    "                h = (np.random.rand(p) < rbm.sortie_entree_RBM(h)) * 1\n",
    "\n",
    "            images.append(h)\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333bc94f-b2f1-4831-bafb-ce3db109df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, size_img = lire_alpha_digit('../data/binaryalphadigs.mat', caractere=['A'])\n",
    "\n",
    "dbn_size = [320, 200, 200]\n",
    "dbn = DBN(dbn_size)  # Instance of DBN\n",
    "dbn.train_DBN(X, learning_rate=1e-2, len_batch=100, n_epochs=100, verbose=0)\n",
    "\n",
    "generated_images = dbn.generer_image_DBN(nb_images=10, nb_iter=200)\n",
    "plot_images(generated_images, size_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ec2ee-c30b-4c20-b5a2-970a0693fe9e",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62d255-860b-4d30-bc9b-7e4e85a11ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_MNIST(file_path='../data/train-images-idx3-ubyte'):\n",
    "    images = load_idx3_ubyte(file_path)\n",
    "    \n",
    "    size_img = images[0].shape\n",
    "    images = images.reshape((images.shape[0],size_img[0]*size_img[1]))\n",
    "    \n",
    "    images = np.round(images/255) # binary MNIST\n",
    "    \n",
    "    return images, size_img\n",
    "\n",
    "def one_hot_encoding(labels, nb_classes):\n",
    "    \"\"\"\n",
    "    Converts a label vector into a one-hot encoded matrix.\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((len(labels), nb_classes))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "def calcul_softmax(X):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.ndarray)\n",
    "    \"\"\"\n",
    "    assert isinstance(X, np.ndarray), \"Please use array.\"\n",
    "    \n",
    "    if X.ndim == 1:\n",
    "        return np.exp(X)/np.sum(np.exp(X))\n",
    "    elif X.ndim == 2:\n",
    "        return np.exp(X)/np.sum(np.exp(X), axis=1, keepdims=True)\n",
    "    else:    \n",
    "        raise ValueError(\"1 or 2 dimensional array.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3439bf67-39d5-46b8-972d-1b30d17f3aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self, network_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            network_size: size of the network including the layer for classification\n",
    "        \"\"\"\n",
    "        self.dbn = DBN(network_size[:-1])\n",
    "        nb_classes = network_size[-1]\n",
    "        self.W_l = np.random.randn(network_size[-2], nb_classes) * np.sqrt(0.01)  # Weights for classification layer\n",
    "        self.b_l = np.zeros(nb_classes)  # Bias for classification layer\n",
    "\n",
    "\n",
    "    def pretrain_DNN(self, X, learning_rate, len_batch, n_epochs, verbose=1):\n",
    "        self.dbn.train_DBN(X, learning_rate, len_batch, n_epochs, verbose)\n",
    "\n",
    "    def entree_sortie_reseau(self, X):\n",
    "        \"\"\"\n",
    "        Store and return inputs + the outputs of each layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        outputs = [X]\n",
    "        \n",
    "        h = X.copy()\n",
    "        for rbm in self.dbn.rbms:\n",
    "            h = rbm.entree_sortie_RBM(h)\n",
    "            outputs.append(h)\n",
    "\n",
    "        y_hat = calcul_softmax(h @ self.W_l + self.b_l)\n",
    "        outputs.append(y_hat)\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "    def retropropagation(self, X, labels, learning_rate, len_batch, n_epochs, verbose=1):\n",
    "        \"\"\"\n",
    "        Perform backpropagation to fine-tune the DBN using labels.\n",
    "        Args:\n",
    "            labels (np.ndarray): one-hot encoded labels.\n",
    "        \"\"\"\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "    \n",
    "        for epoch in range(n_epochs):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = labels[indices]\n",
    "    \n",
    "            for ith_batch in range(0, n_samples, len_batch):\n",
    "\n",
    "                X_batch = X_shuffled[ith_batch:ith_batch + len_batch]\n",
    "                y_batch = y_shuffled[ith_batch:ith_batch + len_batch]\n",
    "\n",
    "                m = X_batch.shape[0]\n",
    "    \n",
    "                # Forward pass\n",
    "                outputs = self.entree_sortie_reseau(X_batch)\n",
    "                y_hat = outputs[-1]\n",
    "                \n",
    "                # Classification layer gradients\n",
    "                c = y_hat - y_batch\n",
    "\n",
    "                grad_w = outputs[-2].T @ c / m \n",
    "                grad_b = np.mean(c, axis=0) \n",
    "                \n",
    "                self.W_l -= learning_rate * grad_w\n",
    "                self.b_l -= learning_rate * grad_b\n",
    "\n",
    "                # Backward pass through hidden layers\n",
    "                weight = self.W_l.copy()\n",
    "                for l in range(len(outputs)-2, 0, -1):\n",
    "                    x = outputs[l]\n",
    "                    c = (c @ weight.T) * (x * (1 - x))\n",
    "\n",
    "                    x_prev = outputs[l-1]\n",
    "                    grad_w = x_prev.T @ c / m\n",
    "                    grad_b = np.mean(c, axis=0)\n",
    "\n",
    "                    rbm = self.dbn.rbms[l-1]\n",
    "                    rbm.W -= learning_rate * grad_w\n",
    "                    rbm.b -= learning_rate * grad_b\n",
    "                    weight = rbm.W\n",
    "\n",
    "            # Reconstruction of the Cross-entropy loss\n",
    "            outputs = self.entree_sortie_reseau(X_shuffled)\n",
    "            y_hat = outputs[-1]\n",
    "\n",
    "            # Cross-entropy loss\n",
    "            loss = -np.sum(y_shuffled * np.log(y_hat)) / n_samples\n",
    "            \n",
    "            if epoch % 10 == 0 and verbose:             \n",
    "                print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss:.4f}\")\n",
    "                \n",
    "    def test_DNN(self, X, labels):\n",
    "        \"\"\"\n",
    "        Compute the error rate.\n",
    "        \n",
    "        Args:\n",
    "            labels (np.ndarray).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Retrieve estimated label\n",
    "        outputs = self.entree_sortie_reseau(X)\n",
    "        y_hat = outputs[-1]\n",
    "        label_estimated = np.argmax(y_hat, axis=1)\n",
    "\n",
    "        errors = (labels != label_estimated).astype(int)\n",
    "        return np.mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0c699-7362-41dd-96f6-20a95a54b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "train_images, train_size_img = process_images_MNIST('../data/train-images-idx3-ubyte')\n",
    "train_labels = load_idx3_ubyte('../data/train-labels-idx1-ubyte')\n",
    "encoded_train_labels = one_hot_encoding(train_labels, nb_classes)\n",
    "\n",
    "test_images, test_size_img = process_images_MNIST('../data/t10k-images-idx3-ubyte')\n",
    "test_labels = load_idx3_ubyte('../data/t10k-labels-idx1-ubyte')\n",
    "\n",
    "nb_features = train_size_img[0]*train_size_img[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a89f7e-4cab-4316-a9f2-a40daf1b21cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_pretrained = DNN(network_size=[nb_features, 600, 600, 600, nb_classes])\n",
    "\n",
    "print(\"----------------------------------------------------- Pre-training -----------------------------------------------------\")\n",
    "dnn_pretrained.pretrain_DNN(train_images,learning_rate=1e-2, len_batch=100, n_epochs=100, verbose=0)\n",
    "print(\"-------------------------------------------------- Back-Propragation ---------------------------------------------------\")\n",
    "dnn_pretrained.retropropagation(train_images, encoded_train_labels, learning_rate=1e-2, len_batch=100, n_epochs=200, verbose=0)\n",
    "print(\"----------------------------------------------------- Error Rate -------------------------------------------------------\")\n",
    "error_rate = dnn_pretrained.test_DNN(test_images, test_labels)\n",
    "print(f\"Error rate: {error_rate*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37576487-a57c-40ba-89d0-928d192022e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = DNN(network_size=[nb_features, 200, 200, nb_classes])\n",
    "\n",
    "print(\"-------------------------------------------------- Back-Propragation ---------------------------------------------------\")\n",
    "dnn.retropropagation(train_images, encoded_train_labels, learning_rate=1e-2, len_batch=100, n_epochs=200, verbose=0)\n",
    "print(\"----------------------------------------------------- Error Rate -------------------------------------------------------\")\n",
    "error_rate = dnn.test_DNN(test_images, test_labels)\n",
    "print(f\"Error rate: {error_rate*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48aadf-e830-40dd-a5f2-0b7084156586",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df1b44-e7c4-452f-882c-108da9a478de",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "train_images, train_size_img = process_images_MNIST('../data/train-images-idx3-ubyte')\n",
    "train_labels = load_idx3_ubyte('../data/train-labels-idx1-ubyte')\n",
    "encoded_train_labels = one_hot_encoding(train_labels, nb_classes)\n",
    "\n",
    "test_images, test_size_img = process_images_MNIST('../data/t10k-images-idx3-ubyte')\n",
    "test_labels = load_idx3_ubyte('../data/t10k-labels-idx1-ubyte')\n",
    "\n",
    "nb_features = train_size_img[0]*train_size_img[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78279580-fb12-49fd-adc9-d0054b8b6341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dnn(network_size, train_images, encoded_train_labels, pretrain=True):\n",
    "    dnn = DNN(network_size=network_size)\n",
    "    if pretrain:\n",
    "        dnn.pretrain_DNN(train_images, learning_rate=1e-2, len_batch=10, n_epochs=10, verbose=0)\n",
    "    dnn.retropropagation(train_images, encoded_train_labels, learning_rate=1e-2, len_batch=10, n_epochs=20, verbose=0)\n",
    "    return dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0110ef-456b-4666-bd63-32399cb5c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_study = pd.DataFrame(columns=['Layers', 'Neurons', 'Train data', 'Error Train', 'Error Test'])\n",
    "\n",
    "nb_layers_dbn = np.arange(1,6)\n",
    "tmp = pd.DataFrame(data={'Layers': nb_layers_dbn, 'Neurons': [200]*len(nb_layers_dbn), 'Train data': [len(train_images)]*len(nb_layers_dbn)})\n",
    "to_study = pd.concat([to_study, tmp], axis=0, ignore_index=True)\n",
    "\n",
    "nb_neurons_dbn = np.arange(1,8)*100\n",
    "tmp = pd.DataFrame(data={'Layers': [2]*len(nb_neurons_dbn), 'Neurons': nb_neurons_dbn, 'Train data': [len(train_images)]*len(nb_neurons_dbn)})\n",
    "to_study = pd.concat([to_study, tmp], axis=0, ignore_index=True)\n",
    "\n",
    "nb_train_data = [1000, 3000, 7000, 10000, 30000, 60000]\n",
    "tmp = pd.DataFrame(data={'Layers': [2]*len(nb_train_data), 'Neurons': [200]*len(nb_train_data), 'Train data': nb_train_data})\n",
    "to_study = pd.concat([to_study, tmp], axis=0, ignore_index=True)\n",
    "\n",
    "to_study.drop_duplicates(subset=[\"Layers\", \"Neurons\", \"Train data\"], ignore_index=True, inplace=True)\n",
    "to_study.sort_values([\"Train data\", \"Layers\", \"Neurons\"], ignore_index=True, inplace = True)\n",
    "\n",
    "to_study[\"Pre-trained\"] = False\n",
    "\n",
    "to_study = pd.concat([to_study, to_study], ignore_index=True)\n",
    "to_study.loc[len(to_study)//2:, \"Pre-trained\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9a579-e040-425e-b3bf-2efce9575e7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, row in to_study.iterrows():\n",
    "    nb_layers_dbn = row[\"Layers\"]\n",
    "    nb_neurons_dbn = row[\"Neurons\"]\n",
    "    nb_data = row[\"Train data\"]\n",
    "    pretrain = row[\"Pre-trained\"]\n",
    "\n",
    "    network_size = [nb_features] + [nb_neurons_dbn for i in range(nb_layers_dbn)] + [nb_classes]\n",
    "\n",
    "    trained_dnn = train_dnn(network_size, train_images[:nb_data], encoded_train_labels[:nb_data], pretrain=pretrain)\n",
    "    \n",
    "    error_rate_train = trained_dnn.test_DNN(train_images, train_labels)\n",
    "    error_rate_test = trained_dnn.test_DNN(test_images, test_labels)\n",
    "    \n",
    "    to_study.iloc[i, 3] = error_rate_train*100\n",
    "    to_study.iloc[i, 4] = error_rate_test*100\n",
    "\n",
    "    if i%5 == 0:    \n",
    "        print(f\"Run - {i}/{len(to_study)-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eaefe5-172c-4453-8404-dd9b445018db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = to_study.query(\"`Pre-trained` == True and Neurons == 200 and `Train data` == 60000\")\n",
    "without_pretrained = to_study.query(\"`Pre-trained` == False and Neurons == 200 and `Train data` == 60000\")\n",
    "\n",
    "nb_layers_dbn = pretrained[\"Layers\"].values\n",
    "\n",
    "e_pretrained_dnn_train = pretrained[\"Error Train\"].values\n",
    "e_pretrained_dnn_test = pretrained[\"Error Test\"].values\n",
    "e_dnn_train = without_pretrained[\"Error Train\"].values\n",
    "e_dnn_test = without_pretrained[\"Error Test\"].values\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axs[0].plot(nb_layers_dbn, e_pretrained_dnn_train, label='Pre-trained DNN', marker='o', color='blue')\n",
    "axs[0].set_xlabel('Number of Hidden Layers in DBN')\n",
    "axs[0].set_ylabel('Pre-Trained - Error Rate (%)', color='blue')\n",
    "axs[0].tick_params(axis='y', labelcolor='blue')\n",
    "axs[0].set_title('Train')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs_twin_train = axs[0].twinx()\n",
    "axs_twin_train.plot(nb_layers_dbn, e_dnn_train, label='Standard DNN', marker='o', color='orange')\n",
    "axs_twin_train.set_ylabel('Standard - Error Rate (%)', color='orange')\n",
    "axs_twin_train.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "axs[1].plot(nb_layers_dbn, e_pretrained_dnn_test, label='Pre-trained DNN', marker='o', color='blue')\n",
    "axs[1].set_xlabel('Number of Hidden Layers in DBN')\n",
    "axs[1].set_ylabel('Pre-Trained - Error Rate (%)', color='blue')\n",
    "axs[1].tick_params(axis='y', labelcolor='blue')\n",
    "axs[1].set_title('Test')\n",
    "axs[1].grid(True)\n",
    "\n",
    "axs_twin_test = axs[1].twinx()  \n",
    "axs_twin_test.plot(nb_layers_dbn, e_dnn_test, label='Standard DNN', marker='o', color='orange')\n",
    "axs_twin_test.set_ylabel('Standard - Error Rate (%)', color='orange')\n",
    "axs_twin_test.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "fig.suptitle('Number of Layers in DBN', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.88)\n",
    "\n",
    "plt.savefig('img/Nb_Layers.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d07a4b-e4fa-4e54-864d-b755171f1ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = to_study.query(\"`Pre-trained` == True and Layers == 2 and `Train data` == 60000\")\n",
    "without_pretrained = to_study.query(\"`Pre-trained` == False and Layers == 2 and `Train data` == 60000\")\n",
    "\n",
    "nb_neurons_dbn = pretrained[\"Neurons\"].values\n",
    "\n",
    "e_pretrained_dnn_train = pretrained[\"Error Train\"].values\n",
    "e_pretrained_dnn_test = pretrained[\"Error Test\"].values\n",
    "e_dnn_train = without_pretrained[\"Error Train\"].values\n",
    "e_dnn_test = without_pretrained[\"Error Test\"].values\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axs[0].plot(nb_neurons_dbn, e_pretrained_dnn_train, label='Pre-trained DNN', marker='o', color='blue')\n",
    "axs[0].set_xlabel('Number of Neurons in DBN')\n",
    "axs[0].set_ylabel('Pre-Trained - Error Rate (%)', color='blue')\n",
    "axs[0].tick_params(axis='y', labelcolor='blue')\n",
    "axs[0].set_title('Train')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs_twin_train = axs[0].twinx()\n",
    "axs_twin_train.plot(nb_neurons_dbn, e_dnn_train, label='Standard DNN', marker='o', color='orange')\n",
    "axs_twin_train.set_ylabel('Standard - Error Rate (%)', color='orange')\n",
    "axs_twin_train.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "axs[1].plot(nb_neurons_dbn, e_pretrained_dnn_test, label='Pre-trained DNN', marker='o', color='blue')\n",
    "axs[1].set_xlabel('Number of Neurons in DBN')\n",
    "axs[1].set_ylabel('Pre-Trained - Error Rate (%)', color='blue')\n",
    "axs[1].tick_params(axis='y', labelcolor='blue')\n",
    "axs[1].set_title('Test')\n",
    "axs[1].grid(True)\n",
    "\n",
    "axs_twin_test = axs[1].twinx()  \n",
    "axs_twin_test.plot(nb_neurons_dbn, e_dnn_test, label='Standard DNN', marker='o', color='orange')\n",
    "axs_twin_test.set_ylabel('Standard - Error Rate (%)', color='orange')\n",
    "axs_twin_test.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "fig.suptitle('Number of Neurons in DBN', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.88)\n",
    "\n",
    "plt.savefig('img/Nb_Neurons.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ded68-b681-4792-aa2a-faf678a1638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = to_study.query(\"`Pre-trained` == True and Layers == 2 and Neurons == 200\")\n",
    "without_pretrained = to_study.query(\"`Pre-trained` == False and Layers == 2 and Neurons == 200\")\n",
    "\n",
    "nb_data = pretrained[\"Train data\"].values\n",
    "\n",
    "e_pretrained_dnn_train = pretrained[\"Error Train\"].values\n",
    "e_pretrained_dnn_test = pretrained[\"Error Test\"].values\n",
    "e_dnn_train = without_pretrained[\"Error Train\"].values\n",
    "e_dnn_test = without_pretrained[\"Error Test\"].values\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axs[0].plot(nb_data, e_pretrained_dnn_train, label='Pre-trained DNN', marker='o', color='blue')\n",
    "axs[0].set_xlabel('Number of Data')\n",
    "axs[0].set_ylabel('Pre-Trained - Error Rate (%)', color='blue')\n",
    "axs[0].tick_params(axis='y', labelcolor='blue')\n",
    "axs[0].set_title('Train')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs_twin_train = axs[0].twinx()\n",
    "axs_twin_train.plot(nb_data, e_dnn_train, label='Standard DNN', marker='o', color='orange')\n",
    "axs_twin_train.set_ylabel('Standard - Error Rate (%)', color='orange')\n",
    "axs_twin_train.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "axs[1].plot(nb_data, e_pretrained_dnn_test, label='Pre-trained DNN', marker='o', color='blue')\n",
    "axs[1].set_xlabel('Number of Data')\n",
    "axs[1].set_ylabel('Pre-Trained - Error Rate (%)', color='blue')\n",
    "axs[1].tick_params(axis='y', labelcolor='blue')\n",
    "axs[1].set_title('Test')\n",
    "axs[1].grid(True)\n",
    "\n",
    "axs_twin_test = axs[1].twinx()  \n",
    "axs_twin_test.plot(nb_data, e_dnn_test, label='Standard DNN', marker='o', color='orange')\n",
    "axs_twin_test.set_ylabel('Standard - Error Rate (%)', color='orange')\n",
    "axs_twin_test.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "fig.suptitle('Number of Data', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.88)\n",
    "\n",
    "plt.savefig('img/Nb_Data.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
